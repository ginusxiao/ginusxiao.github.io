# 提纲
[toc]

## Master（YB-Master）简介
- keeper of system metadata and records
    - what tables exist in the system and where their tablets live
    - what users and roles exist and the permission associated with them
    - and so on
- and is responsible for coordinating background operations
    - Data initial-placement and load balancing
    - Leader balancing
        - Aside from ensuring that the number of tablets served by each YB-TServer is balanced across the universe, the YB-Masters also ensures that each node has a symmetric number of tablet-peer leaders across eligible nodes
    - such as load-balancing or initiating re-replication of under-replicated data
- and is responsible for perfoming a variety of administrative operations
    - such as creating, altering, and dropping tables
- is highly available as it forms a Raft group with its peers 
    - ![image](https://docs.yugabyte.com/images/architecture/master_overview.png)

## Master启动相关参数整理
### log，data，wal存储路径相关的设置
- --log_dir (If specified, logfiles are written into this directory instead of
      the default logging directory.) type: string default: ""
- --fs_data_dirs (Comma-separated list of data directories. This argument must
      be specified.) type: string default: ""
- --fs_wal_dirs (Comma-separated list of directories for write-ahead logs.
      This is an optional argument. If this is not specified, fs_data_dirs is
      used for write-ahead logs also and that's a reasonable default for most
      use cases.) type: string default: ""

### wal持久化相关的设置
- --require_durable_wal_write (Whether durable WAL write is required.In case
      you cannot write using O_DIRECT in WAL and data directories and this flag
      is set truethe system will deliberately crash with the appropriate error.
      If this flag is set false, the system will soft downgrade the
      durable_wal_write flag.) type: bool default: false
- --durable_wal_write (Whether the Log/WAL should explicitly call fsync()
      after each write.) type: bool default: false currently: true

### Master raft group中参与节点的地址设置
- --master_addresses (Comma-separated list of the host/port RPC addresses of
      the peer masters. This is needed for initial cluster create, and is
      recreated from persisted metadata on master restart.This flag also
      defaults to empty, which is overloaded to be able to either: a) start a
      non-distributed mode master if local instance file is not present. b)
      allow for a master to be restarted gracefully and get its peer list from
      the local cmeta file of the last committed config, if local instance file
      is present.) type: string default: ""
- --master_replication_factor (Number of master replicas. By default it is
      detected based on master_addresses option, but could be specified
      explicitly together with passing one or more master service domain name
      and port through master_addresses for masters auto-discovery when running
      on Kubernetes.) type: uint64 default: 0

### CallHome相关的设置
- -callhome_collection_level (Level of details sent by callhome) type: string
      default: "medium"
- -callhome_enabled (Enables callhome feature that sends analytics data to
      yugabyte) type: bool default: true
- -callhome_interval_secs (How often to run callhome) type: int32
      default: 3600
- -callhome_tag (Tag to be inserted in the json sent to FLAGS_callhome_url.
      This tag is used by itest to specify that the data generated by callhome
      should be discarded by the receiver.) type: string default: ""
- -callhome_url (URL of callhome server) type: string
      default: "http://diagnostics.yugabyte.com"

## Master启动流程
### MasterMain入口
1. 借助于google gflags中提供的ParseCommandLineNonHelpFlags来进行参数解析，解析后的参数都是类似于FLAGS_***的形式。
2. 检查通过--log_dir，--fs_data_dirs和--fs_wal_dirs指定的路径是否支持O_DIRECT，如果任何一个不支持O_DIRECT，则：若require_durable_wal_write参数设置为true，则打印提示信息并退出，若require_durable_wal_write参数设置为false，则修改durable_wal_write参数为true。
3. 创建Log目录。如果没有指定--log_dir参数的话，则直接在--fs_data_dirs参数中指定的以逗号分隔的多个目录中的第一个目录下创建一个名为yb-data/master/logs/的目录，创建成功之后，设置FLAGS_log_dir；如果指定了--log_dir参数的话，则直接使用FLAGS_log_dir中的值。
4. 初始化Master 配置项(MasterOptions)。主要是检查--master_addresses和--master_replication_factor这两个参数的设置是否匹配。如果不匹配则提示错误信息并退出，否则创建一个MasterOptions实例，并设置master_addresses_flag（MasterOptions中的成员域）和master_addresses_（MasterOptions的父类ServerBaseOptions中的成员域）参数。
5. 创建Master实例（直接调用Master的构造方法，参数为步骤4中的MasterOptions），具体见“创建Master实例”。
6. 初始化Master，具体见“初始化Master”。
7. 启动Master，具体见“启动Master”。
8. 启动CallHome，用于周期性发送集群相关的统计信息到YugaByte Diagnostic Service，默认的YugaByte Diagnostics Service网址是http://diagnostics.yugabyte.com，可以通过--callhome_url设置之，关于CallHome的作用及其相关的设置见[Diagnostics reporting](https://docs.yugabyte.com/latest/manage/diagnostics-reporting/#root)。
9. 创建内存使用情况监视器（TotalMemWatcher）。

### 创建Master实例
1. Master类的继承关系为：Master -> RpcAndWebServerBase -> RpcServerBase，各类的主要成员如下：
```
class Master : public server::RpcAndWebServerBase {
  ..
  # 监控TServer的心跳信息
  gscoped_ptr<TSManager> ts_manager_;
  # 管理tables和tablets的状态和位置信息
  gscoped_ptr<enterprise::CatalogManager> catalog_manager_;
  # web page处理相关
  gscoped_ptr<MasterPathHandlers> path_handlers_;
  # table和tablet数据flush相关
  gscoped_ptr<FlushManager> flush_manager_;
  # Master配置项
  MasterOptions opts_;
  # 负责调度一些后台操作，比如flush和compaction
  std::shared_ptr<MaintenanceManager> maintenance_manager_;
  # Master自身的TabletServer，用于管理Master自身相关的tables
  std::unique_ptr<MasterTabletServer> master_tablet_server_;
  ..
}

class RpcAndWebServerBase : public RpcServerBase {
  ..
  # 对YugaByte中数据和元数据相关的目录结构，及其目录中文件读写接口的封装
  gscoped_ptr<FsManager> fs_manager_;
  # 对Squeasel web server library的封装
  gscoped_ptr<Webserver> web_server_;
  ..
}

class RpcServerBase {
  ..
  # RPC服务
  gscoped_ptr<RpcServer> rpc_server_;
  # RPC事件处理
  std::unique_ptr<rpc::Messenger> messenger_;
  # RPC代理
  std::unique_ptr<rpc::ProxyCache> proxy_cache_;
  ...
}
```

2. 创建Master实例相关的代码如下：
```
Master::Master(const MasterOptions& opts)
  : RpcAndWebServerBase(
        "Master", opts, "yb.master", server::CreateMemTrackerForServer()),
    state_(kStopped),
    ts_manager_(new TSManager()),
    catalog_manager_(new enterprise::CatalogManager(this)),
    path_handlers_(new MasterPathHandlers(this)),
    flush_manager_(new FlushManager(this, catalog_manager())),
    opts_(opts),
    registration_initialized_(false),
    maintenance_manager_(new MaintenanceManager(MaintenanceManager::DEFAULT_OPTIONS)),
    metric_entity_cluster_(METRIC_ENTITY_cluster.Instantiate(metric_registry_.get(),
                                                             "yb.cluster")),
    master_tablet_server_(new MasterTabletServer(this, metric_entity())) {
  SetConnectionContextFactory(rpc::CreateConnectionContextFactory<rpc::YBInboundConnectionContext>(
      GetAtomicFlag(&FLAGS_inbound_rpc_memory_limit),
      mem_tracker()));
}
```
通过调用父类RpcAndWebServerBase的构造方法完成RpcAndWebServerBase相关的初始化，然后就是通过Master成员域对应的构造方法创建各成员域，其中一句代码SetConnectionContextFactory(rpc::CreateConnectionContextFactory<rpc::YBInboundConnectionContext>(GetAtomicFlag(&FLAGS_inbound_rpc_memory_limit), mem_tracker()))会创建一个ConnectionContextFactory，并以此初始化RpcServerBase::rpc_server_。

### 初始化Master
1. 创建线程池Master::init_pool_，该线程池专用于初始化catalog manager。
2. 初始化RpcAndWebServerBase。见“初始化RpcAndWebServerBase”。
3. 将Master::path_handlers_(类型为MasterPathHandlers，与Master web page相关)注册到RpcAndWebServerBase::web_server_(类型为Webserver)中。在MasterPathHandlers中定义了所有与Master web page处理相关的方法。

#### 初始化RpcAndWebServerBase
1. 打开FsManager。主要包括以下步骤：
- 初始化FsManager。
    - 检查data_fs_roots_（来自于--fs_data_dirs参数）是否被设置，若没有设置，则提示错误并退出。
    - 检查（如指定的路径是空字符串，或者不是绝对路径，或者包含非法的空白字符）并规范化wal_fs_roots_（来自于--fs_wal_dirs参数，如果--fs_wal_dirs未指定，则来自于--fs_data_dirs参数）和data_fs_roots_中指定的所有的路径，最后形成FsManager::canonicalized_wal_fs_roots_，FsManager::canonicalized_metadata_fs_root_（只有data_fs_roots_中的第一个目录会扮演该角色）和FsManager::canonicalized_data_fs_roots_以及FsManager::canonicalized_all_fs_roots_（data_fs_roots_和wal_fs_roots_中的所有规范化后的文件的集合）。
    - 检查FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下是否存在名为fs-lock的文件，如果任何一个目录下存在这样的文件，则表明文件系统可能存在不一致状态，提示错误并退出。
    - 检查FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下是否存在名为instance的文件，并分如下两路进行执行：
        - 如果存在，则读取该文件中保存的关于Master的metadata信息，并以此来设置FsManager::metadata_；如果任何两个目录下的yb-data/master/目录下是否存在名为instance的文件中记录的metadata信息中记录的uuid不一致，提示错误并退出；
        - 如果不存在则创建新的文件系统布局（这里指的是Master相关的目录和文件）：
            - 检查data_fs_roots_（来自于--fs_data_dirs参数）是否被设置，若没有设置，则提示错误并退出。
            - 检查（如指定的路径是空字符串，或者不是绝对路径，或者包含非法的空白字符）并规范化wal_fs_roots_（来自于--fs_wal_dirs参数，如果--fs_wal_dirs未指定，则来自于--fs_data_dirs参数）和data_fs_roots_中指定的所有的路径，最后形成FsManager::canonicalized_wal_fs_roots_，FsManager::canonicalized_metadata_fs_root_（只有data_fs_roots_中的第一个目录会扮演该角色）和FsManager::canonicalized_data_fs_roots_以及FsManager::canonicalized_all_fs_roots_（data_fs_roots_和wal_fs_roots_中的所有规范化后的文件的集合）。
            - 检查FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下是否存在名为fs-lock的文件，如果任何一个目录下存在这样的文件，则删除当前已存在的文件系统布局。
            - 在FsManager::canonicalized_all_fs_roots_中的所有目录下创建yb-data/master/目录。
            - 在FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下创建fs-lock文件。
            - 设置Master的metadata信息（类型为InstanceMetadataPB），包括uuid，time string和hostname等。
            - 在FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下创建instance文件，并将Master的metadata信息写入其中。
            - 在FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下创建wals目录、tablet-meta目录和consensus-meta目录。
            - 如果--enable_data_block_fsync标志被设置，则确保所有新创建的目录都同步到文件系统。
            - 在FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下创建data目录，并在该目录下创建名为rocksdb的目录。
            - 删除FsManager::canonicalized_all_fs_roots_中的所有目录下的yb-data/master/目录下的fs-lock文件。
            - 重新打开FsManager。
    - 经过上述步骤之后，Master相关的文件系统布局如下：
    ```
    ***/yb-data/master/
    ├── consensus-meta
    ├── data
    │   └── rocksdb
    ├── instance
    ├── tablet-meta
    └── wals
    ```
- 初始化RpcServerBase，这里会涉及到RPC server，RPC proxy，RPC messenger，这3者之间的关系是什么样的呢？见另一篇博文：YugaByte中RPC相关组件分析
    - 初始化混合时钟。
    - 创建RPC messager。
    - 创建RPC proxy cache。proxy cache中保存的是从{Host+Port, protocol} -> RPC proxy的映射关系，
    - 初始化RPC server

2. 初始化RpcServerBase，这里会涉及到RPC server，RPC proxy，RPC messenger，这3者之间的关系是什么样的呢？见另一篇博文：[YugaByte中RPC相关组件分析](http://note.youdao.com/noteshare?id=ab27b2c0831b93f24c53ee377171a4e6&sub=674100C14C1A4C24B95F40F96BC7480A)
    - 初始化混合时钟。
    - 借助于MessengerBuilder来创建RPC messager。
        - 其中一个主要步骤是创建多个reactor线程。
    - 创建RPC proxy cache。proxy cache中保存的是从{Host+Port, protocol} -> RPC proxy的映射关系，其中{Host+Port, protocol}整体作为ProxyKey。
    - 初始化RPC server。
        - 设置RpcServer::messenger_.
        - 从参数中解析出RpcServer::rpc_host_port_和RpcServer::rpc_bind_addresses_，并对RpcServer::rpc_bind_addresses_中的地址对应的端口号进行检查。
        - 设置RpcServer::server_state_为INITIALIZED。
    - 将RPC server绑定到RpcServer::rpc_bind_addresses_中的地址列表上去。
        - 为RpcServer::messenger_创建一个Messenger::acceptor_，并监听在RpcServer::rpc_bind_addresses_中的每一个地址上。
        - 设置RpcServer::server_state_为BOUND。

### 启动Master
1. 初始化Master::maintenance_manager_。主要是启动一个名为maintenance_scheduler的线程，并通过MaintenanceManager::monitor_thread_引用之，该线程的运行主体是MaintenanceManager::RunSchedulerThread()。
2. 注册Master相关的RPC服务。见“注册Master相关的RPC服务”。
3. 启动RpcAndWebServerBase。见“启动RpcAndWebServerBase”。
4. 初始化Master registration protobuf，其中包括rpc address，http address，broadcast address，存放的位置信息（在哪个cloud的哪个region的哪个zone上）等信息。
5. 初始化catalog manager。见“初始化catalog manager”。

#### 注册Master相关的RPC服务
注册Master相关的RPC服务方法为：Master::RegisterServices()，这里注册了master_service，master_tablet_service，consensus_service，remote_bootstrap_service等服务，对于各种服务都是先创建一个相关的服务实例，然后调用RpcAndWebServerBase::RegisterService注册之。各RPC服务用途说明如下：

master_service：服务类型为MasterServiceImpl，类继承关系为MasterServiceImpl -> (MasterServiceIf, MasterServiceBase)，在MasterServiceIf中定义了该服务相关的接口，在MasterServiceImpl中则实现了该接口。table，namespace和role等的创建更新和删除操作都由该服务提供。

master_tablet_service：服务类型为MasterTabletServiceImpl，类的继承关系为MasterTabletServiceImpl -> TabletServiceImpl -> TabletServerServiceIf，其中TabletServerServiceIf中定义了Tablet操作相关的接口，如Tablet读写相关的接口，获取Tablets列表相关的接口等等；TabletServiceImpl则为TServer实现了Tablet操作相关的接口；而MasterTabletServiceImpl则实现了Master上Tablet操作相关的接口。

consensus_service：服务类型为ConsensusServiceImpl，类的继承关系为ConsensusServiceImpl -> ConsensusServiceIf，其中ConsensusServiceIf中定义了consensus操作相关的接口，ConsensusServiceImpl中则实现了这些接口。

remote_bootstrap_service：服务类型为RemoteBootstrapServiceImpl，类的继承关系为RemoteBootstrapServiceImpl -> RemoteBootstrapServiceImpl，其中RemoteBootstrapServiceImpl定义了Remote bootstrap相关的接口，RemoteBootstrapServiceImpl中则实现了这些接口。

相关代码：
```
Status Master::RegisterServices() {
  # 注册master_service
  std::unique_ptr<ServiceIf> master_service(new MasterServiceImpl(this));
  RETURN_NOT_OK(RpcAndWebServerBase::RegisterService(FLAGS_master_svc_queue_length,
                                                     std::move(master_service)));
  # 注册master_tablet_service
  std::unique_ptr<ServiceIf> master_tablet_service(
      new MasterTabletServiceImpl(master_tablet_server_.get(), this));
  RETURN_NOT_OK(RpcAndWebServerBase::RegisterService(FLAGS_master_tserver_svc_queue_length,
                                                     std::move(master_tablet_service)));
  # 注册consensus_service
  std::unique_ptr<ServiceIf> consensus_service(
      new ConsensusServiceImpl(metric_entity(), catalog_manager_.get()));
  RETURN_NOT_OK(RpcAndWebServerBase::RegisterService(FLAGS_master_consensus_svc_queue_length,
                                                     std::move(consensus_service),
                                                     rpc::ServicePriority::kHigh));
  # 注册remote_bootstrap_service
  std::unique_ptr<ServiceIf> remote_bootstrap_service(
      new RemoteBootstrapServiceImpl(fs_manager_.get(), catalog_manager_.get(), metric_entity()));
  RETURN_NOT_OK(RpcAndWebServerBase::RegisterService(FLAGS_master_remote_bootstrap_svc_queue_length,
                                                     std::move(remote_bootstrap_service)));
  return Status::OK();
}
```

#### 启动RpcAndWebServerBase
- 为Master上的RPC server产生一个instance ID，并记录在RpcServerBase::instance_pb_中。
- 注册各种web page path handler到RpcAndWebServerBase::web_server_中。
    - 注册一些默认的web page path handler到RpcAndWebServerBase::web_server_中。
    - 注册RPC 相关的web page path handler到RpcAndWebServerBase::web_server_中。
    - 注册metrics相关的web page path handler到RpcAndWebServerBase::web_server_中。
    - 注册tracing相关的web page path handler到RpcAndWebServerBase::web_server_中。
- 设置RpcAndWebServerBase::web_server_的footer html。
- 启动RpcAndWebServerBase::web_server_，在此不详解。
- 启动RpcServerBase。
    - 注册GenericService，顾名思义，一些RPC相关的通用服务（相关接口定义在GenericServiceImpl中），如ping之类的。
    - 检查RPC server当前的状态，如果是INITIALIZED状态，则先执行Bind操作，其实就是在给定的RPC监听地址上进行监听；在Bind操作过程中会修改状态为BOUND。
    - 修改状态为STARTED。
    - RpcServer::messenger_启动Acceptor线程，该线程的执行主体为Acceptor::RunThread，它会不断的监听并处理新的连接请求。

#### 初始化catalog manager
- 初始化两个metrics信息，分别为：metric_num_tablet_servers_live_和metric_num_tablet_servers_dead_。
- 初始化SysCatalogTable，SysCatalogTable是YugaByte中的一个特殊的table，它只有一个tablet，用于记录table和tablet的元数据信息。
    - 构建SysCatalogTable的schema：entry_type, entry_id, metadata。其中entry_type和entry_id形成组合键，entry_type表示当前这个entry是一个table还是一个tablet，entry_id则表示当前table或者tablet对应的id。
    - 从SysCatalogTable的schema构建SysCatalogTable的partitionSchema：所谓的paritionSchema是指table中的每一行数据在该table的所有的tablets中是如何分布的，PartitionSchema的作用主要就是用于将主键列的值转换为partition key，以此来决定一行数据存在于哪个tablet中。
    - 创建分区信息。分区信息存放在Partition中，Partition用于描述率属于某个Tablet的所有的行，每个Tablet都对应一个Partition，Partiton中主要包括分区的起始key和终止key。因为SysCatlogTable只有一个Tablet，所以最终也只有一个Partition。
    - 为syscatalog tablet创建RaftGroupMetadata。
        - 检查${FsManager::canonicalized_metadata_fs_root_}/yb-data/master/tablet-meta下是否存在给定Tablet（这里是SysCatalogTablet，其对应的Tablet id是固定的00000000000000000000000000000000）的raft group metadata文件，如果已存在，则提示错误，否则继续。
        - 通过FsManager获取所有的数据的根目录列表data_root_dirs和wal的根目录列表wal_root_dirs，并在数据的根目录列表中随机挑选一个目录（该目录记为data_top_dir）存放SysCatalog table和tablet相关的数据，在wal的根目录列表中随机挑选一个目录（该目录记为wal_top_dir）存放SysCatalog table和tablet相关的wal。
        - 设置该syscatalog tablet对应的wal目录为${wal_top_dir}/yb-data/master/wal/table-{syscatalog_table_id}/tablet-{syscatalog_tablet_id}。
        - 设置该syscatalog tablet对应的数据目录为${data_top_dir}/yb-data/master/data/rocksdb/table-{syscatalog_table_id}/tablet-{syscatalog_tablet_id}。
        - 构建RaftGroupMetadata实例，其中包括以下信息：raft group id，KV-store(其中包含对应的rocksdb目录，有哪些table共享了该KV-store等)，FsManager，wal目录等。
        - 通过RaftGroupMetadata构建RaftGroupReplicaSuperBlockPB。
        - 将RaftGroupReplicaSuperBlockPB写入文件${FsManager::canonicalized_metadata_fs_root_}/yb-data/master/tablet-meta/00000000000000000000000000000000中。
    - 为syscatalog tablet创建consensus metadata。
        - 创建ConsensusMetadata。
        - 检查目录${FsManager::canonicalized_metadata_fs_root_}/yb-data/master/consensus-meta是否存在，如果不存在则创建之。
        - 将ConsensusMetadata转换为ConsensusMetadataPB并写入文件${FsManager::canonicalized_metadata_fs_root_}/yb-data/master/consensus-meta/00000000000000000000000000000000中。
    - 根据RaftGroupMetadata启动Tablet。这里可能是启动一个新的Tablet，也可能是启动一个旧的Tablet，如果是启动旧的Tablet，则会进行日志重放，我们暂时只关注启动一个新的Tablet的情况。
        - 从文件${FsManager::canonicalized_metadata_fs_root_}/yb-data/master/consensus-meta/00000000000000000000000000000000中读取ConsensusMetadata信息。
        - 构造一个Tablet实例，并打开该Tablet。
            - 直接调用Tablet的构造方法来构造Tablet实例。
            - 打开Tablet包括如下步骤：
                - 初始化rocksdb::Options。
                - 确保yb-data/master/data/rocksdb/table-{syscatalog_table_id}/tablet-{syscatalog_tablet_id}目录存在，如果不存在，则创建之。
                - 确保yb-data/master/data/rocksdb/table-{syscatalog_table_id}/tablet-{syscatalog_tablet_id}.intents目录存在，如果不存在，则创建之。
                - 打开regular db。
                - 打开provisional db。
        - 检查SysCatalog tablet对应的tablet wal path：${wal_top_dir}/yb-data/master/wal/table-{syscatalog_table_id}/tablet-{syscatalog_tablet_id}是否存在，如果不存在，则创建之。
        - 构造一个Log实例，用于操作SysCatalog tablet wal log。
        - 创建一个新的wal log segment，并切换到该wal log segment上。
        - 初始化wal log appender（用于写wal log）。
    - 初始化Tablet peer。
        - 创建RaftConsensus实例。
        - 创建并启动Preparer线程。
        - 启动该Tablet peer所对应的tablet的transaction coordinator。
        - 启动该Tablet peer所对应的tablet的transaction participant。
    - 启动Tablet peer。
        - 启动RaftConsensus。
        - 在对应的tablet的regular db和provisional db上开启compaction。
    - 在Master::maintenance_manager_上注册一个Log GC task。
- 启动CatalogManager后台任务，任务运行主体为CatalogManagerBgTasks::Run()，它的主要工作是监听CreateTable或者AssignmentTimeout事件，其中CreateTable用于进行Tablet Assignment，而AssignmentTimeout事件表示CatalogManager指定在某个TServer上创建Tablet时过了超时时间仍没有接收到成功创建Tablet的响应。关于CatalogManagerBgTasks，请参考[这里](https://github.com/cloudera/kudu/blob/master/docs/design-docs/master.md)。

经过上述步骤之后，Master相关的文件系统布局如下：
```
├── consensus-meta      # ConsensusMetadata目录，每个tablet都在该目录下面对应一个文件
│   └── 00000000000000000000000000000000
├── data                # 数据目录，先按照table进行组织，在按照tablet进行组织，table下面的每一个tablet会对应两个目录，分别为regular db对应的rocksdb目录和provisional db对应的rocksdb目录
│   └── rocksdb
│       └── table-sys.catalog.uuid      # SysCatalog table ID
│           ├── tablet-00000000000000000000000000000000             # SysCatalog tablet对应的regular db
│           │   ├── 000010.sst
│           │   ├── 000010.sst.sblock.0
│           │   ├── 000019.log
│           │   ├── CURRENT
│           │   ├── IDENTITY
│           │   ├── LOCK
│           │   ├── MANIFEST-000018
│           │   ├── OPTIONS-000023
│           │   └── OPTIONS-000025
│           ├── tablet-00000000000000000000000000000000.intents     # SysCatalog tablet对应的provisional db
│           │   ├── 000009.log
│           │   ├── CURRENT
│           │   ├── IDENTITY
│           │   ├── LOCK
│           │   ├── MANIFEST-000008
│           │   ├── OPTIONS-000013
│           │   └── OPTIONS-000015
│           └── tablet-00000000000000000000000000000000.snapshots   # SysCatalog tablet对应的snapshot文件
├── instance    
├── logs    # 日志数据
│   ├── yb-master.INFO -> yb-master.localhost.localdomain.root.log.INFO.20191231-071649.14945
│   ├── yb-master.localhost.localdomain.root.log.INFO.20191231-071649.14945
│   ├── yb-master.localhost.localdomain.root.log.WARNING.20191231-071649.14945
│   └── yb-master.WARNING -> yb-master.localhost.localdomain.root.log.WARNING.20191231-071649.14945
├── tablet-meta     # RaftGroupMetadata目录，每个tablet在该目录下对应一个文件，存放其RaftGroupMetadata信息
│   └── 00000000000000000000000000000000    # SysCatalog tablet
└── wals            # wal log目录，先按照table进行组织，table内部则按照tablet进行组织
    └── table-sys.catalog.uuid              # SysCatalog table ID
        └── tablet-00000000000000000000000000000000                 # SysCatalog tablet ID
            ├── index.000000000
            ├── wal-000000001
            └── wal-000000002
```


## 参考
[kudu master design](https://github.com/cloudera/kudu/blob/master/docs/design-docs/master.md)

[Maintenance Op Scheduling](https://github.com/cloudera/kudu/blob/master/docs/design-docs/triggering-maintenance-ops.md)


